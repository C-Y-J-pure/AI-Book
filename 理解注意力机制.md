**将注意力集中在最重要的部分，忽略其他不太重要的信息注意力集中在最重要的部分，忽略其他不太重要的信息**将注意力集中在最重要的部分，忽略其他不太重要的信息注意力机制在神经网络中的作用类似，它可以帮助模型集中于输入数据中的关键信息，从而提高模型的性能和解释性。
**出现的背景、动机：**
解决传统神经网络模型处理序列数据时，模型需要将整个序列编码成一个固定长度的向量，然后再进行解码等操作。这些操作存在的问题：
- 信息丢失：当序列很长时，无法很好地理解序列中所有关键词汇和语义。
- 难以捕捉长距离依赖关系：序列中距离较远的元素之间传统的编码方式难以捕捉。
**注意力机制**就是为了解决这些问题而提出的，它能够让模型在处理序列中的每个元素时，**动态地关注整个序列**中与当前元素最相关的信息。

### 基本原理
1. **核心**：计算一个注意力权重分布，然后根据权重分布来加权求和序列中的信息，从而得到一个加权后的表示。
2. **计算注意力分数（Attention Scores）**：对于输入序列中的每个元素（如单词、字符等），计算它和其他所有元素之间的相关性得分。这些得分反映了不同元素之间的关注度。
3. **计算注意力权重（Attention Weights）**：将注意力分数通过softmax函数进行归一化，得到注意力权重。softmax函数可以将分数转换为概率分布，使得所有权重之和为1。这样，每个元素对应一个权重，权重越大表示关注度越高。
4. **加权求和得到上下文向量（Context Vector）**：根据注意力权重，对输入序列中的元素进行加权求和，得到一个上下文向量。这个上下文向量融合了与当前元素最相关的信息。



![[Pasted image 20250314233844.png]]